import asyncio
from contextlib import contextmanager
from enum import StrEnum
from pathlib import Path
from typing import Annotated, Generator, List

import rich
from typer import Option, Typer

from common.config import DatabaseConfig
from queue_populator.config import QueuePopulatorConfig
from queue_populator.lambda_handler import populate_sqs_queue
from seeding.export_to_s3 import run_s3_export
from seeding.restore import run_s3_restore
from service_migration.application import DataMigrationApplication, DMSEvent
from service_migration.config import (
    DataMigrationConfig,
)
from service_migration.processor import ServiceTransformOutput

CONSOLE = rich.get_console()


class TargetEnvironment(StrEnum):
    local = "local"
    dev = "dev"
    test = "test"
    int = "int"
    sandpit = "sandpit"
    ref = "ref"


typer_app = Typer(
    name="dos-etl",
    help="DoS Data Migration Pipeline CLI",
)


@typer_app.command("migrate")
def migrate_handler(  # noqa: PLR0913
    db_uri: Annotated[
        str | None, Option(..., help="URI to connect to the source database")
    ],
    env: Annotated[str, Option(..., help="Environment to run the migration in")],
    workspace: Annotated[
        str | None, Option(help="Workspace to run the migration in")
    ] = None,
    ddb_endpoint_url: Annotated[
        str | None, Option(help="URL to connect to local DynamoDB")
    ] = None,
    service_id: Annotated[
        str | None, Option(help="Service ID to migrate (for single record sync)")
    ] = None,
    output_dir: Annotated[
        Path | None, Option(help="Directory to save transformed records (dry run only)")
    ] = None,
) -> None:
    """
    Local entrypoint for testing the data migration.
    This function can be used to run the full or single sync process locally.
    """
    app = DataMigrationApplication(
        config=DataMigrationConfig(
            db_config=DatabaseConfig.from_uri(db_uri),
            ENVIRONMENT=env,
            WORKSPACE=workspace,
            ENDPOINT_URL=ddb_endpoint_url,
        ),
    )

    with patch_local_save_method(app, output_dir):
        if service_id:
            event = DMSEvent(
                type="dms_event",
                record_id=service_id,
                method="insert",
                table_name="services",
            )
            app.handle_dms_event(event)
        else:
            app.handle_full_sync_event()


@typer_app.command("populate-queue")
def populate_queue_handler(
    db_uri: Annotated[str, Option(..., help="URI to connect to the source database")],
    sqs_queue_url: Annotated[
        str, Option(..., help="SQS queue URL to populate with legacy services")
    ],
    type_id: Annotated[
        List[int] | None, Option(help="List of type IDs to filter services by")
    ] = None,
    status_id: Annotated[
        List[int] | None, Option(help="List of status IDs to filter services by")
    ] = None,
) -> None:
    """
    Local entrypoint for populating the queue with legacy services.
    This function can be used to test the queue population logic.
    """
    config = QueuePopulatorConfig(
        db_config=DatabaseConfig.from_uri(db_uri),
        SQS_QUEUE_URL=sqs_queue_url,
        type_ids=type_id,
        status_ids=status_id,
    )
    populate_sqs_queue(config)


@contextmanager
def patch_local_save_method(
    app: DataMigrationApplication, output_dir: Path | None
) -> Generator:
    """
    Patch the application to save transformed records to a local directory.
    This is useful for testing without affecting the database.
    """
    if output_dir is None:
        yield
        return

    output_dir.mkdir(parents=True, exist_ok=True)
    organisation_path = output_dir / "organisation.jsonl"
    location_path = output_dir / "location.jsonl"
    healthcare_path = output_dir / "healthcare-service.jsonl"

    organisation_file = open(organisation_path, "w")
    location_file = open(location_path, "w")
    healthcare_file = open(healthcare_path, "w")

    def _mock_save(result: ServiceTransformOutput) -> None:
        organisation_file.writelines(
            org.model_dump_json() + "\n" for org in result.organisation
        )
        location_file.writelines(
            loc.model_dump_json() + "\n" for loc in result.location
        )
        healthcare_file.writelines(
            hc.model_dump_json() + "\n" for hc in result.healthcare_service
        )

    app.processor._save = _mock_save
    yield

    organisation_file.close()
    location_file.close()
    healthcare_file.close()


@typer_app.command("export-to-s3")
def export_to_s3_handler(
    env: Annotated[str, Option(..., help="Environment to run the export in")],
    workspace: Annotated[
        str | None, Option(..., help="Workspace to run the export in")
    ] = None,
) -> None:
    """
    Handler for exporting data from all DynamoDB tables to S3.
    """
    asyncio.run(run_s3_export(env, workspace))


@typer_app.command("restore-from-s3")
def restore_from_s3_handler(
    env: Annotated[str, Option(..., help="Environment to run the restore in")],
    workspace: Annotated[
        str | None, Option(..., help="Workspace to run the restore in")
    ] = None,
) -> None:
    """
    Handler for restoring data from S3 to all DynamoDB tables.
    """
    asyncio.run(run_s3_restore(env, workspace))
